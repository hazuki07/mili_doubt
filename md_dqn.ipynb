{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hazuk\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gym\\core.py:318: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
      "c:\\Users\\hazuk\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
     ]
    }
   ],
   "source": [
    "import env.env as env\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "env = gym.make('MillionDoubtEnv-v0').unwrapped\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "リプレイメモリを管理するクラス。\n",
    "メモリに格納する経験データ（transition）のタプルをnamedtupleで定義。\n",
    "\n",
    "通常は、経験データは{現在の状態、選択した行動、次の状態、報酬}であるが、~~効率化のために次の状態の合法手の一覧も格納するようにする~~。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Replay Memory\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# DQN\n",
    "# 10層の畳み込みニューラルネットワークを全結合層に接続。\n",
    "# 活性化関数: tanh >> 行動価値(-1 ~ 1)を出力\n",
    "\n",
    "# TODO k...特徴量の合計サイズ\n",
    "k = 74\n",
    "fcl_units = 256\n",
    "num_actions = sum((act.n) for act in env.action_space.values())\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, k, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(k)\n",
    "        self.conv2 = nn.Conv2d(k, k, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(k)\n",
    "        self.conv3 = nn.Conv2d(k, k, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(k)\n",
    "        self.conv4 = nn.Conv2d(k, k, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(k)\n",
    "        self.conv5 = nn.Conv2d(k, k, kernel_size=3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(k)\n",
    "        self.conv6 = nn.Conv2d(k, k, kernel_size=3, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(k)\n",
    "        self.conv7 = nn.Conv2d(k, k, kernel_size=3, padding=1)\n",
    "        self.bn7 = nn.BatchNorm2d(k)\n",
    "        self.conv8 = nn.Conv2d(k, k, kernel_size=3, padding=1)\n",
    "        self.bn8 = nn.BatchNorm2d(k)\n",
    "        self.conv9 = nn.Conv2d(k, k, kernel_size=3, padding=1)\n",
    "        self.bn9 = nn.BatchNorm2d(k)\n",
    "        self.conv10 = nn.Conv2d(k, k, kernel_size=3, padding=1)\n",
    "        self.bn10 = nn.BatchNorm2d(k)\n",
    "        # TODO\n",
    "        self.fcl1 = nn.Linear(k * 74, fcl_units)\n",
    "        self.fcl2 = nn.Linear(fcl_units, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        x = F.relu(self.bn6(self.conv6(x)))\n",
    "        x = F.relu(self.bn7(self.conv7(x)))\n",
    "        x = F.relu(self.bn8(self.conv8(x)))\n",
    "        x = F.relu(self.bn9(self.conv9(x)))\n",
    "        x = F.relu(self.bn10(self.conv10(x)))\n",
    "        # TODO \n",
    "        # print(x.shape)\n",
    "        x = F.relu(self.fcl1(x.view(-1, k * 74)))\n",
    "        x = self.fcl2(x)\n",
    "        return x.tanh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obs_to_tensor(obs):\n",
    "\n",
    "    player_hand_tensor = torch.tensor(obs['player_hand'], dtype=torch.float32).view(1, 28)\n",
    "    opponent_hand_len_tensor = torch.tensor([obs['opponent_hand_len']], dtype=torch.float32).view(1, 1)\n",
    "    field_tensor = torch.tensor(obs['field'], dtype=torch.float32).view(1, 39)\n",
    "    phase_type_tensor = torch.tensor([obs['phase_type']], dtype=torch.float32).view(1, 1)\n",
    "    is_revolution_tensor = torch.tensor([obs['is_revolution']], dtype=torch.float32).view(1, 1)\n",
    "    restricted_suits_tensor = torch.tensor([obs['restricted_suits']], dtype=torch.float32).view(1, 4)\n",
    "    \n",
    "    # Concatenate all tensors along the second dimension to create a single input tensor\n",
    "    input_tensor = torch.cat((player_hand_tensor, opponent_hand_len_tensor, field_tensor, phase_type_tensor, is_revolution_tensor, restricted_suits_tensor), dim=1)\n",
    "    state = input_tensor.view(1, 1, 2, 37).to(device)\n",
    "\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Training\n",
    "# 訓練に使用するハイパーパラメータの設定。ニューラルネットワーク、オプティマイザ、リプレイメモリの初期化\n",
    "\n",
    "# εグリーディー方策で選ぶ関数を定義\n",
    "\n",
    "from re import L\n",
    "\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 2000\n",
    "OPTIMIZE_PER_EPISODES = 16\n",
    "TARGET_UPDATE = 4\n",
    "\n",
    "policy_net = DQN().to(device)\n",
    "target_net = DQN().to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters(), lr=1e-5)\n",
    "\n",
    "memory = ReplayMemory(131072)\n",
    "\n",
    "def epsilon_greedy(state, legal_moves):\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * episodes_done / EPS_DECAY)\n",
    "\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            q = policy_net(state)\n",
    "            # print(q.shape) # NOTE\n",
    "            if any(isinstance(sublist, list) for sublist in legal_moves):\n",
    "                legal_moves = convert_legal_moves(legal_moves)\n",
    "            print(legal_moves)\n",
    "            _, select = q[0, legal_moves].max(0)\n",
    "    else:\n",
    "        select = random.randrange(len(legal_moves))\n",
    "        \n",
    "    return select\n",
    "\n",
    "# TODO dict\n",
    "def select_action(obs, game, player):\n",
    "    action = {\n",
    "        'play_card': np.empty([0]),\n",
    "        'play_card_back': np.empty([0]),\n",
    "        'doubt': np.empty([0]),\n",
    "        'select_card': np.empty([0]),\n",
    "    }\n",
    "    \n",
    "    legal_moves = []\n",
    "    state = obs_to_tensor(obs)\n",
    "    \n",
    "    if obs['phase_type'] == 1:\n",
    "        legal_moves = game.searching_legal_move(player)\n",
    "        \n",
    "        select = epsilon_greedy(state, legal_moves)\n",
    "        selected_move = legal_moves[select]\n",
    "        action['play_card'] = selected_move[0]\n",
    "        action['play_card_back'] = selected_move[1]\n",
    "        selected_move = convert_array(selected_move)\n",
    "        selected_move.append([0] * 13)\n",
    "        selected_move.append([0] * 13)\n",
    "\n",
    "# TODO\n",
    "    elif obs['phase_type'] == 2:\n",
    "        legal_moves = [0, 1]\n",
    "        \n",
    "        select = epsilon_greedy(state, legal_moves)\n",
    "        action['doubt'] = legal_moves[select]\n",
    "        selected_move = [[0] * 13]\n",
    "        selected_move.append([0] * 13)\n",
    "        converted_move = [legal_moves[select]]\n",
    "        converted_move.extend([0] * 12)\n",
    "        selected_move.append(converted_move)\n",
    "        selected_move.append([0] * 13)\n",
    "\n",
    "# select card(sample())\n",
    "    elif obs['phase_type'] == 3:\n",
    "        indexed_list = list(range(len(game.field)))\n",
    "        for i in range(len(game.field)):\n",
    "            for subset in combinations(indexed_list, i):\n",
    "                legal_moves.append(list(subset))\n",
    "\n",
    "        select = epsilon_greedy(state, legal_moves)\n",
    "        selected_move = legal_moves[select]\n",
    "        action['select_card'] = selected_move\n",
    "        selected_move = [[0] * 13]\n",
    "        selected_move.append([0] * 13)\n",
    "        selected_move.append([0] * 13)\n",
    "        converted_move = convert_array(legal_moves[select])\n",
    "        selected_move.append(converted_move)\n",
    "    \n",
    "    else:\n",
    "        selected_move = [[0] * 13]\n",
    "        selected_move.append([0] * 13)\n",
    "        selected_move.append([0] * 13)\n",
    "        selected_move.append([0] * 13)\n",
    "        \n",
    "        print(\"NotImplemented\")\n",
    "\n",
    "    # print(obs['phase_type']) # NOTE\n",
    "    # print(selected_move) # NOTE\n",
    "    tensor_move = torch.tensor(selected_move, device=device, dtype=torch.long)\n",
    "    \n",
    "    return action, tensor_move\n",
    "\n",
    "def convert_array(input_list):\n",
    "    if any(isinstance(sublist, list) for sublist in input_list):\n",
    "    # 1つ目のサブリストの変換\n",
    "        first_sublist = [1 if i in input_list[0] else 0 for i in range(13)]\n",
    "    \n",
    "    # 2つ目のサブリストの変換\n",
    "        second_sublist = [first_sublist[i] for i in input_list[1]]\n",
    "        second_sublist = [1 if i in second_sublist else 0 for i in range(13)]\n",
    "    \n",
    "        return [first_sublist, second_sublist]\n",
    "    \n",
    "    else:\n",
    "        lst = [1 if i in input_list else 0 for i in range(13)]\n",
    "    \n",
    "    return lst\n",
    "\n",
    "def flatten_nested_list(lst):\n",
    "    flat_list = []\n",
    "    for item in lst:\n",
    "        if isinstance(item, list):\n",
    "            flat_list.extend(item)\n",
    "        else:\n",
    "            flat_list.append(item)\n",
    "    return flat_list\n",
    "\n",
    "def convert_legal_moves(lst):\n",
    "    combined_list = [flatten_nested_list(sublist) for sublist in lst]\n",
    "    converted_list = [int(\"0\".join(map(str, sublist))) if sublist else 0 for sublist in combined_list]\n",
    "    return converted_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Training loop\n",
    "\n",
    "losses = []\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # 合法手のみ\n",
    "    non_final_next_actions_list = []\n",
    "    for next_actions in batch.next_actions:\n",
    "        if next_actions is not None:\n",
    "            non_final_next_actions_list.append(next_actions + [next_actions[0]] * (30 - len(next_actions)))\n",
    "    non_final_next_actions = torch.tensor(non_final_next_actions_list, device=device, dtype=torch.long)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    # 合法手のみの最大値\n",
    "    target_q = target_net(non_final_next_states)\n",
    "    # 相手番の価値のため反転する\n",
    "    next_state_values[non_final_mask] = -target_q.gather(1, non_final_next_actions).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = next_state_values * GAMMA + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hazuk\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe27798a18ed40eaa39543b972bda343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\マイドライブ\\ファイル\\Python\\a\\mili_doubt\\env\\env.py:213: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  elif action['doubt']:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your atk turn.\n",
      "[♠6, ♠K]\n",
      "MLPlayer(MLPlayer)_1の操作\n",
      " \n",
      "[♡J, ♣Q]\n",
      "[0, 1]\n",
      "MLPlayer(MLPlayer)_0の操作\n",
      " \n",
      "[♢6, ♢10]\n",
      "MLPlayer(MLPlayer)_1の操作\n",
      "ダウト成功\n",
      "NotImplemented\n",
      " \n",
      "[♠7, ♠A]\n",
      "MLPlayer(MLPlayer)_0の操作\n",
      " \n",
      "[0, 10001, 20001, 1020001]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4704\\4037459854.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m# Select and perform an action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mturn\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmy_phase\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"play\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mturn\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmy_phase\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"play\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0mmove\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplayer0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m             \u001b[0mnext_obs0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmove\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4704\\4114322537.py\u001b[0m in \u001b[0;36mselect_action\u001b[1;34m(obs, game, player)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mselect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepsilon_greedy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlegal_moves\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0mselected_move\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlegal_moves\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[0maction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'play_card'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselected_move\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0maction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'play_card_back'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselected_move\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# main training loop\n",
    "\n",
    "num_episodes = 10000\n",
    "episodes_done = 0\n",
    "pbar = tqdm(total=num_episodes)\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "\n",
    "    for t in count():\n",
    "        obs0, obs1 = env.update_obs()\n",
    "        next_obs0 = []\n",
    "        next_obs1 = []\n",
    "        \n",
    "        # Select and perform an action\n",
    "        if (env.game.turn and env.game.my_phase == \"play\") or (not env.game.turn and not env.game.my_phase == \"play\"):\n",
    "            move, action = select_action(obs0, env.game, env.game.player0)\n",
    "            next_obs0, reward0, done, info = env.step(move)\n",
    "\n",
    "            reward0 = torch.tensor([reward0], device=device)\n",
    "            \n",
    "            # Store the transition in memory\n",
    "            memory.push(obs0, action, next_obs0, reward0)\n",
    "        else:\n",
    "            move, action = select_action(obs1, env.game, env.game.player0)\n",
    "            next_obs1, reward1, done, info = env.step(move)\n",
    "\n",
    "            reward1 = torch.tensor([reward1], device=device)\n",
    "            \n",
    "            # memory.push(obs0, action, next_obs0, reward0)\n",
    "\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        # Move to the next state\n",
    "        obs0 = next_obs0\n",
    "        obs1 = next_obs1\n",
    "\n",
    "    episodes_done += 1\n",
    "    pbar.update()\n",
    "\n",
    "    if i_episode % OPTIMIZE_PER_EPISODES == OPTIMIZE_PER_EPISODES - 1:\n",
    "        # Perform several episodes of the optimization (on the target network)\n",
    "        optimize_model()\n",
    "\n",
    "        pbar.set_description(f'loss = {losses[-1]:.3e}')\n",
    "\n",
    "        # Update the target network, copying all weights and biases in DQN\n",
    "        if i_episode // OPTIMIZE_PER_EPISODES % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "modelfile = 'model.pt'\n",
    "print('save {}'.format(modelfile))\n",
    "torch.save({'state_dict': target_net.state_dict(), 'optimizer': optimizer.state_dict()}, modelfile)\n",
    "\n",
    "print('Complete')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
